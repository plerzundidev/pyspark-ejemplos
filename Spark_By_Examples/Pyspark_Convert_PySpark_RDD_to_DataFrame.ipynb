{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnV248zIq+rGXpMezYcu+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plerzundidev/pyspark-ejemplos/blob/main/Pyspark_Convert_PySpark_RDD_to_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Iniciamos librerias esenciales para su funcionamiento"
      ],
      "metadata": {
        "id": "u4GKcKS8W4Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Levantamos google drive para ejecutar el instalador\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0LJpHrzW5k4",
        "outputId": "d0d4e816-4a9e-4775-8c8e-937da8c42aa3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora iniciamos pyspark\n",
        "exec(open('/content/drive/MyDrive/BigDataSw/spark_colab_installer_new.py').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuRQ8KNVW7aD",
        "outputId": "aec0ac50-ff9f-4d60-ee11-8b32d0f683cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Servicios Activos:\n",
            "5697 Jps\n",
            "3713 SparkSubmit\n",
            "1779 NameNode\n",
            "2073 JobHistoryServer\n",
            "1930 ResourceManager\n",
            "5514 DataNode\n",
            "1997 NodeManager\n",
            "\n",
            "Apache Spark installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En PySpark, la función toDF() del RDD se utiliza para convertir RDD a DataFrame. Necesitaríamos convertir RDD a DataFrame ya que DataFrame proporciona más ventajas sobre RDD. Por ejemplo, DataFrame es una colección distribuida de datos organizados en columnas con nombre similar a las tablas de bases de datos y proporciona optimización y mejoras de rendimiento."
      ],
      "metadata": {
        "id": "CtEZHGCDYRGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Crear PySpark RDD\n",
        "\n",
        "En primer lugar, vamos a crear un RDD pasando el objeto de lista Python a la función sparkContext.parallelize(). Necesitaremos este objeto rdd para todos nuestros ejemplos a continuación.\n",
        "\n",
        "En PySpark, cuando tienes datos en una lista significa que tienes una colección de datos en la memoria de un controlador PySpark cuando creas un RDD, esta colección va a ser paralelizada."
      ],
      "metadata": {
        "id": "bmFGVOkOYVOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# esencial para que el contenedor reconozca la instalacion de pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
      ],
      "metadata": {
        "id": "7i4Oaa7IXE-y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos un rdd\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)"
      ],
      "metadata": {
        "id": "bffwcQbMYmUk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQsSENIqaU7h",
        "outputId": "34da9961-7106-4bb9-b186-c765e58f171f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Finance', 10), ('Marketing', 20), ('Sales', 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Convertir PySpark RDD a DataFrame\n",
        "\n",
        "La conversión de PySpark RDD a DataFrame se puede hacer usando toDF(), createDataFrame(). En esta sección, voy a explicar estos dos métodos."
      ],
      "metadata": {
        "id": "VhbPAVXMaSd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Utilización de la función rdd.toDF()\n",
        "\n",
        "PySpark proporciona la función toDF() en RDD que se puede utilizar para convertir RDD en Dataframe\n",
        "\n",
        "Por defecto, la función toDF() crea nombres de columna como \"_1\" y \"_2\". Este fragmento produce el siguiente esquema."
      ],
      "metadata": {
        "id": "2XnYQSOdaqwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH9MZC-0aAbK",
        "outputId": "e7a3c51b-d11f-4a30-ed0d-b2aa84fc3bc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "toDF() tiene otra firma que toma argumentos para definir los nombres de las columnas como se muestra a continuación.\n",
        "\n",
        "Salida del siguiente esquema."
      ],
      "metadata": {
        "id": "pzUGfydpbkwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V2oilfLa4RL",
        "outputId": "2bec63e2-70c9-4735-a70f-08cdc6beae95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Uso de la función createDataFrame() de PySpark\n",
        "La clase SparkSession proporciona el método createDataFrame() para crear el DataFrame y toma el objeto rdd como argumento."
      ],
      "metadata": {
        "id": "6kYb1EGEcNWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1QWOzezboDw",
        "outputId": "38a9ad5f-88dd-4f8c-d45b-99ac2f3a9b33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Utilización de createDataFrame() con el esquema StructType\n",
        "\n",
        "Cuando se infiere el esquema, por defecto el tipo de datos de las columnas se deriva de los datos y se establece nullable a true para todas las columnas. Podemos cambiar este comportamiento proporcionando el esquema utilizando StructType - donde podemos especificar un nombre de columna, tipo de datos y nullable para cada campo/columna.\n",
        "\n",
        "Si desea saber más acerca de StructType, por favor vaya a través de cómo utilizar StructType y StructField para definir el esquema personalizado."
      ],
      "metadata": {
        "id": "7Nl8IpFEdWhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])\n",
        "\n",
        "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
        "deptDF1.printSchema()\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwFxomFvc8og",
        "outputId": "8f533956-367e-42e0-aa57-d070b3b062fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: string (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M29ZWSFqdqba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}