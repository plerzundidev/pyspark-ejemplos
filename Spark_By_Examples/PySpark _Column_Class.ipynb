{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d27ce9c-5200-4bee-9d07-b61e3fd8253c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La clase pyspark.sql.Column proporciona varias funciones para trabajar con DataFrame para manipular los valores de la columna, evaluar la expresión booleana para filtrar filas, recuperar un valor o parte de un valor de una columna DataFrame, y para trabajar con columnas list, map & struct.\n",
    "\n",
    "En este artículo, voy a cubrir cómo crear objetos Column, acceder a ellos para realizar operaciones, y, finalmente, las Funciones Column más utilizadas de PySpark con Ejemplos.\n",
    "\n",
    "Artículo Relacionado: Clase Row de PySpark con Ejemplos\n",
    "\n",
    "**Puntos Clave:**\n",
    "\n",
    "La clase Column de PySpark representa una única Columna en un DataFrame.\n",
    "Proporciona las funciones más utilizadas para manipular Columnas y Filas de un DataFrame.\n",
    "Algunas de estas funciones de Columna evalúan una expresión Booleana que puede ser utilizada con la transformación filter() para filtrar las Filas del DataFrame.\n",
    "Proporciona funciones para obtener un valor de una columna de lista por índice, mapear valor por clave & índice, y finalmente struct nested column.\n",
    "PySpark también proporciona funciones adicionales pyspark.sql.functions que toman un objeto Column y devuelven un tipo Column.\n",
    "**Nota:** La mayoría de las funciones pyspark.sql.functions devuelven el tipo Column, por lo que es muy importante conocer las operaciones que se pueden realizar con el tipo Column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bfae4e-eab7-49b3-9cfe-757d49218da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. Crear un Objeto de Clase Columna\n",
    "\n",
    "Una de las formas más sencillas de crear un objeto de clase Columna es utilizando la función SQL lit() de PySpark, esta toma un valor literal y devuelve un objeto Columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647fbbce-9eb5-400a-8754-40f19ccc017c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(\"sparkbyexamples.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028b033e-cbfe-41f7-a17a-072b5d070348",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()\n",
    "#root\n",
    "# |-- name.fname: string (nullable = true)\n",
    "# |-- gender: long (nullable = true)\n",
    "\n",
    "# Using DataFrame object (df)\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dc3b8f5-5849-4bc4-bf3a-855548a2cf1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente ejemplo muestra el acceso a columnas de tipo struct. Aquí he utilizado la clase PySpark Row para crear un tipo struct. Alternativamente, también se puede crear mediante el uso de PySpark StructType & StructField clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae40fc6f-6200-42c7-a034-344f6a73be32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black| blue|\n| grey|black|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Create DataFrame with struct using Row class\n",
    "from pyspark.sql import Row\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "#root\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- prop: struct (nullable = true)\n",
    "# |    |-- hair: string (nullable = true)\n",
    "# |    |-- eye: string (nullable = true)\n",
    "\n",
    "#Access struct column\n",
    "df.select(df.prop.hair).show()\n",
    "df.select(df[\"prop.hair\"]).show()\n",
    "df.select(col(\"prop.hair\")).show()\n",
    "\n",
    "#Access all columns from struct\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0558e592-1e36-4561-b1e0-4f638078b414",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2. Operadores de columna de PySpark\n",
    "PySpark column también proporciona una forma de hacer operaciones aritméticas sobre columnas usando operadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa5de36-a66b-41fa-8e47-1e3a5112b0f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col1 % col2)|\n+-------------+\n|            0|\n|            2|\n|            0|\n+-------------+\n\n+-------------+\n|(col2 > col3)|\n+-------------+\n|         true|\n|        false|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 < col3)|\n+-------------+\n|        false|\n|         true|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 = col3)|\n+-------------+\n|        false|\n|        false|\n|         true|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "#Arthmetic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ec9006-5158-4368-bc8a-4faa1c425955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3. Funciones de Columna de PySpark\n",
    "Veamos algunas de las Funciones de Columna más usadas, en la tabla de abajo, he agrupado las funciones relacionadas para hacerlo más fácil, haz click en el enlace para ver los ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42b4e38d-8367-4c32-b439-0580650ed0bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "| Función de columna          | Descripción de la función                                                                                          |\n",
    "|-----------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| **alias(*alias, kwargs)      | Proporciona un alias a la columna o expresión.                                                                     |\n",
    "| **name(*alias, kwargs)       | Devuelve lo mismo que alias().                                                                                     |\n",
    "| asc()                       | Ordena la columna en orden ascendente.                                                                             |\n",
    "| asc_nulls_first()           | Ordena los valores nulos primero, luego los valores no nulos en orden ascendente.                                  |\n",
    "| asc_nulls_last()            | Ordena los valores nulos después de los valores no nulos en orden ascendente.                                      |\n",
    "| astype(tipoDeDato)          | Convierte el tipo de dato a otro tipo.                                                                            |\n",
    "| cast(tipoDeDato)            | Devuelve lo mismo que astype().                                                                                   |\n",
    "| between(limiteInferior, limiteSuperior) | Comprueba si los valores de la columna están entre el límite inferior y el superior. Devuelve un valor booleano. |\n",
    "| bitwiseAND(otra)            | Calcula el AND bit a bit de esta expresión con otra expresión.                                                    |\n",
    "| bitwiseOR(otra)             | Calcula el OR bit a bit de esta expresión con otra expresión.                                                     |\n",
    "| bitwiseXOR(otra)            | Calcula el XOR bit a bit de esta expresión con otra expresión.                                                    |\n",
    "| contains(otra)             | Comprueba si una cadena contiene otra cadena.                                                                    |\n",
    "| desc()                      | Ordena la columna en orden descendente.                                                                           |\n",
    "| desc_nulls_first()          | Ordena los valores nulos primero, luego los valores no nulos en orden descendente.                                |\n",
    "| desc_nulls_last()           | Ordena los valores nulos después de los valores no nulos en orden descendente.                                    |\n",
    "| startswith(otra)            | Comprueba si una cadena comienza con otra cadena. Devuelve una expresión booleana.                                |\n",
    "| endswith(otra)              | Comprueba si una cadena termina con otra cadena. Devuelve una expresión booleana.                                  |\n",
    "| eqNullSafe(otra)            | Realiza una prueba de igualdad segura para valores nulos.                                                         |\n",
    "| getField(nombre)            | Devuelve un campo por nombre en un StructField y por clave en un Map.                                             |\n",
    "| getItem(clave)              | Devuelve un valor de un Map/Clave en la posición proporcionada.                                                    |\n",
    "| isNotNull()                 | Devuelve True si la expresión actual NO es nula.                                                                  |\n",
    "| isNull()                    | Devuelve True si la expresión actual es nula.                                                                     |\n",
    "| *isin(columnas)             | Devuelve True si el valor de esta expresión está contenido en los valores evaluados de los argumentos.             |\n",
    "| like(otra)                  | Similar a la expresión LIKE de SQL.                                                                               |\n",
    "| rlike(otra)                 | Similar a la expresión RLIKE de SQL (LIKE con Regex).                                                              |\n",
    "| over(ventana)               | Se utiliza con la columna de ventana.                                                                            |\n",
    "| substr(posInicial, longitud) | Devuelve una subcadena de la columna.                                                                            |\n",
    "| when(condición, valor)      | Similar a la expresión CASE WHEN de SQL, ejecuta una lista de condiciones y devuelve una de las múltiples expresiones de resultado posibles. |\n",
    "| otherwise(valor)            | Se usa con when().                                                                                                |\n",
    "| *dropFields(nombresDeCampo) | Elimina campos en un StructType por nombre.                                                                      |\n",
    "| withField(nombreDeCampo, columna) | Agrega o reemplaza un campo en un StructType por nombre.                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f85258f0-a80c-4d21-970d-97aa4a34daad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#4. Ejemplos de Funciones de Columna de PySpark\n",
    "Vamos a crear un simple DataFrame para trabajar con los ejemplos de PySpark SQL Column. En la mayoría de los ejemplos, me referiré al nombre del objeto DataFrame (df.) para obtener la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77aff0de-1238-4503-892e-02db174a4cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a9add0-af34-441c-84eb-f651fde7484b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.1 alias() - Asigna un nombre a la columna\n",
    "En el siguiente ejemplo df.fname se refiere al objeto Columna y alias() es una función de la Columna para dar un nombre alternativo. Aquí, la columna fname ha sido cambiada a first_name & lname a last_name.\n",
    "\n",
    "En el segundo ejemplo he usado la función expr() de PySpark para concatenar columnas y he nombrado la columna como fullName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb304fe4-5513-4857-a2cd-b631ba2539e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullName|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#alias\n",
    "from pyspark.sql.functions import expr\n",
    "df.select(df.fname.alias(\"first_name\"), \\\n",
    "          df.lname.alias(\"last_name\")\n",
    "   ).show()\n",
    "\n",
    "#Another example\n",
    "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302fa27d-8a3a-4a4d-85d5-eaf5852d1bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.2 asc() & desc() - Ordenar las columnas del DataFrame por orden Ascendente o Descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a77b4fbf-d3c7-42ea-81b4-8d95f5f43b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#asc, desc to sort ascending and descending order repsectively.\n",
    "df.sort(df.fname.asc()).show()\n",
    "df.sort(df.fname.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9973c08a-b904-4074-b248-38641c769394",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.3 cast() & astype() - Se utilizan para convertir el Tipo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f169d3b0-6064-4d2f-8742-e7c13767ed75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fname: string (nullable = true)\n |-- id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#cast\n",
    "df.select(df.fname,df.id.cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f46cb0d5-e7e3-4d96-bd54-47f3da54b866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.4 between() - Devuelve una expresión booleana cuando los valores de una columna se encuentran entre los límites inferior y superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347126fa-d9b2-4c32-9a50-60c9cd76d37c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n|  Ann|Varsa|200|     F|\n+-----+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#between\n",
    "df.filter(df.id.between(100,300)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3e54c8-eb6b-4633-a7dc-387000ab4f54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.5 contiene() -\n",
    "Comprueba si un valor de columna de PySpark DataFrame contiene un valor de cadena especificado en esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad1a73b-0db4-449f-b4ad-9ad2fe89c5da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#contains\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2e9308-6ee7-4ca2-bbd3-ead4a1c177ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.6 startswith() & endswith() -\n",
    "Comprueba si el valor de la Columna DataFrame startsWith() y endsWith() es una Cadena. startsWith() filtra las filas en las que existe una subcadena especificada al principio, mientras que endsWith() filtra las filas en las que la subcadena especificada se presenta al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f684bb3-4ce3-4e50-85e6-aa321c30fba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#startswith, endswith()\n",
    "df.filter(df.fname.startswith(\"T\")).show()\n",
    "df.filter(df.fname.endswith(\"Cruise\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84481285-cf16-424a-9bb4-efdd2ab00f88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.7 isNull & isNotNull() - Comprueba si la columna DataFrame tiene valores NULL o no NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c00df7f-a477-427a-85fc-94b2fc33019a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+------+\n|    fname|lname| id|gender|\n+---------+-----+---+------+\n|Tom Brand| null|400|     M|\n+---------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#isNull & isNotNull\n",
    "df.filter(df.lname.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b1ff845-4983-4959-b2bc-98b64adb5caf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.8 like() & rlike() - Similar a la expresión LIKE de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e985bf74-84b3-4fff-b184-8a6f77a07bf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: DataFrame[fname: string, lname: string, id: string]"
     ]
    }
   ],
   "source": [
    "#like , rlike\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.fname.like(\"%om\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b2d990-28ec-454d-bfb1-9573443dc0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.9 substr() - Devuelve una Columna después de obtener la subcadena de la Columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c813a98e-9684-4eab-b5ad-635380c68b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|substr|\n+------+\n|    Ja|\n|    An|\n|    To|\n|    To|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.fname.substr(1,2).alias(\"substr\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7222db-4d6e-482d-9564-ce80bec74f5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.10 when() & otherwise() - Es similar a SQL Case When, ejecuta la secuencia de expresiones hasta que coincide con la condición y devuelve un valor cuando coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14cb1d7-e22d-48c0-b27d-d03f2296cd62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n|     fname|lname|new_gender|\n+----------+-----+----------+\n|     James| Bond|      null|\n|       Ann|Varsa|    Female|\n|Tom Cruise|  XXX|          |\n| Tom Brand| null|      Male|\n+----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#when & otherwise\n",
    "from pyspark.sql.functions import when\n",
    "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
    "              .when(df.gender==\"F\",\"Female\") \\\n",
    "              .when(df.gender==None ,\"\") \\\n",
    "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "260b67c0-88cd-47be-b02c-d9598d8bb71d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.11 isin() - Comprueba si el valor está presente en una Lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa43266-2649-4fdc-9052-879d9db5721c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+\n|     fname|lname| id|\n+----------+-----+---+\n|     James| Bond|100|\n|       Ann|Varsa|200|\n|Tom Cruise|  XXX|400|\n| Tom Brand| null|400|\n+----------+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#isin\n",
    "li=[\"100\",\"200\",\"400\"]\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.id.isin(li)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75df1da1-03a6-4304-96ea-f70cf8907a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.12 getField() - Para obtener el valor por la clave de la columna MapType y por el nombre del conducto hijo de la columna StructType.\n",
    "\n",
    "El resto de las funciones operan sobre estructuras de datos List, Map y Struct, por lo que para demostrarlas usaré otro DataFrame con columnas list, map y struct. Para más explicaciones sobre el uso de Arrays ver PySpark ArrayType Column on DataFrame Examples y para map ver PySpark MapType Examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d40afe7-a0d0-4f12-8ea9-2a343a32d1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Create DataFrame with struct, array & map\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f18712-6c44-461e-b608-ddfe48c306ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n|Tom Cruise|\n| Tom Brand|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#getField from MapType\n",
    "df.select(df.properties.getField(\"hair\")).show()\n",
    "\n",
    "#getField from Struct\n",
    "df.select(df.name.getField(\"fname\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2083d87-7493-4bc0-9244-a36d25e5379b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.13 getItem() - Para obtener el valor por índice de la columna MapType o ArrayTupe & ny key para MapType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459b2003-7ce9-4988-8579-8126b23e59ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|languages[1]|\n+------------+\n|          C#|\n|      Python|\n|       Scala|\n|        Ruby|\n+------------+\n\n+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#getItem() used with ArrayType\n",
    "df.select(df.languages.getItem(1)).show()\n",
    "\n",
    "#getItem() used with MapType\n",
    "df.select(df.properties.getItem(\"hair\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark – Column Class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
