{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b12c06-d856-4a9f-b984-f0bebb1f3df2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Las clases StructType y StructField de PySpark se utilizan para especificar mediante programación el esquema del DataFrame y crear columnas complejas como struct anidadas, array y map columns. StructType es una colección de objetos StructField que define el nombre de la columna, el tipo de datos de la columna, un booleano para especificar si el campo puede ser anulable o no y metadatos.\n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "**Definición de esquemas DataFrame:** StructType se utiliza habitualmente para definir el esquema al crear un DataFrame, sobre todo para datos estructurados con campos de distintos tipos de datos.\n",
    "\n",
    "**Estructuras anidadas:** Se pueden crear esquemas complejos con estructuras anidadas anidando StructType dentro de otros objetos StructType, lo que permite representar datos jerárquicos o multinivel.\n",
    "\n",
    "**Aplicación de la estructura de datos:** Al leer datos de diversas fuentes, especificar un StructType como esquema garantiza que los datos se interpretan y estructuran correctamente. Esto es importante cuando se trata de fuentes de datos semiestructuradas o sin esquema.\n",
    "\n",
    "En este artículo, explicaré diferentes formas de definir la estructura de DataFrame usando StructType con ejemplos de PySpark. Aunque PySpark infiere un esquema a partir de los datos, a veces podemos necesitar definir nuestros propios nombres de columnas y tipos de datos y este artículo explica cómo definir esquemas simples, anidados y complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6331d3-a4c2-467c-b339-2385a20c42fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. StructType - Define la estructura del DataFrame\n",
    "\n",
    "PySpark proporciona la clase StructType de pyspark.sql.types para definir la estructura del DataFrame.\n",
    "StructType es una colección o lista de objetos StructField.\n",
    "El método printSchema() de PySpark en el DataFrame muestra las columnas StructType como struct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408ef4ab-0089-4d27-84e2-d7dbe5bc3eab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2. StructField - Define los metadatos de la columna DataFrame\n",
    "\n",
    "PySpark proporciona la clase pyspark.sql.types import StructField para definir las columnas que incluyen nombre de columna(String), tipo de columna (DataType), columna anulable (Boolean) y metadatos (MetaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0d5e23-93ba-4249-9975-dcbac60b1ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3. Usando PySpark StructType & StructField con DataFrame\n",
    "\n",
    "Al crear un PySpark DataFrame podemos especificar la estructura utilizando las clases StructType y StructField. Como se especifica en la introducción, StructType es una colección de StructField que se utiliza para definir el nombre de la columna, tipo de datos, y una bandera para nullable o no. Usando StructField también podemos añadir esquemas struct anidados, ArrayType para arrays, y MapType para pares clave-valor que discutiremos en detalle en secciones posteriores.\n",
    "\n",
    "El siguiente ejemplo muestra un ejemplo muy simple de cómo crear un StructType & StructField en DataFrame y su uso con datos de ejemplo para soportarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e19d883-80ea-4669-9a23-078db860f576",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, MapType\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c9edec-24b5-4eba-a508-3400ca5f9271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#4. Definir estructura anidada de objetos StructType\n",
    "\n",
    "Cuando trabajamos con DataFrame a menudo necesitamos trabajar con la columna struct anidada y esto se puede definir utilizando StructType.\n",
    "En el siguiente ejemplo, el tipo de datos de la columna \"name\" es StructType, que está anidado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a61d21-e2be-4a2a-afee-be17826b7e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3100  |\n|{Michael, Rose, }   |40288|M     |4300  |\n|{Robert, , Williams}|42114|M     |1400  |\n|{Maria, Anne, Jones}|39192|F     |5500  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Defining schema using nested StructType\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58699d0-f874-4053-9079-376f035af369",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#5. Añadir y cambiar la estructura del DataFrame\n",
    "\n",
    "Usando la función SQL de PySpark struct(), podemos cambiar la estructura del DataFrame existente y añadirle un nuevo StructType. El siguiente ejemplo demuestra cómo copiar las columnas de una estructura a otra y añadir una nueva columna. La clase Column de PySpark también proporciona algunas funciones para trabajar con la columna StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0629f391-9163-47d0-9083-15f8f1204381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- OtherInfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |OtherInfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3100, Medium}|\n|{Michael, Rose, }   |{40288, M, 4300, High}  |\n|{Robert, , Williams}|{42114, M, 1400, Low}   |\n|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Updating existing structtype using struct\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "updatedDF = df2.withColumn(\"OtherInfo\", \n",
    "    struct(col(\"id\").alias(\"identifier\"),\n",
    "    col(\"gender\").alias(\"gender\"),\n",
    "    col(\"salary\").alias(\"salary\"),\n",
    "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "  )).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7396b299-986f-44a9-881b-c6ff986d4065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#6. Uso de SQL ArrayType y MapType\n",
    "\n",
    "SQL StructType también soporta ArrayType y MapType para definir las columnas DataFrame para las colecciones array y map respectivamente. En el siguiente ejemplo, la columna hobbies se define como ArrayType(StringType) y las propiedades se definen como MapType(StringType,StringType) lo que significa que tanto la clave como el valor son String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c464ef-4103-4881-a118-56bbab6f6c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using SQL ArrayType and MapType\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),StringType()), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff702ba7-e502-466b-9155-763ac1f531b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#7. Creación de la estructura del objeto StructType a partir de un archivo JSON\n",
    "\n",
    "Si tienes demasiadas columnas y la estructura del DataFrame cambia de vez en cuando, es una buena práctica cargar el esquema SQL StructType desde un fichero JSON. Puedes obtener el esquema usando df2.schema.json() , almacenarlo en un fichero y usarlo para crear el esquema desde este fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9689489-a723-4997-872d-f3b163c9a426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "# Using json() to load StructType\n",
    "print(df2.schema.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba8e577-584d-4b0c-875a-02eb42e6df86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"name\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"struct\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"firstname\",\n",
    "        \"type\" : \"string\",\n",
    "        \"nullable\" : true,\n",
    "        \"metadata\" : { }\n",
    "      }, {\n",
    "        \"name\" : \"middlename\",\n",
    "        \"type\" : \"string\",\n",
    "        \"nullable\" : true,\n",
    "        \"metadata\" : { }\n",
    "      }, {\n",
    "        \"name\" : \"lastname\",\n",
    "        \"type\" : \"string\",\n",
    "        \"nullable\" : true,\n",
    "        \"metadata\" : { }\n",
    "      } ]\n",
    "    },\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"dob\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"gender\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"salary\",\n",
    "    \"type\" : \"integer\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd881e5-7f21-4ee2-8de2-97ba8e192782",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternativamente, también puede utilizar df.schema.simpleString(), esto devolverá un formato de esquema relativamente más simple.\n",
    "\n",
    "Ahora vamos a cargar el archivo json y utilizarlo para crear un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb6459a-28ae-497d-b017-32934fbf7999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2522813972922994>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0;31m# Loading json schema to create DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 3\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      4\u001B[0m df3 = spark.createDataFrame(\n",
       "\u001B[1;32m      5\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/json/__init__.py\u001B[0m in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n",
       "\u001B[1;32m    337\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    338\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbytes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 339\u001B[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
       "\u001B[0m\u001B[1;32m    340\u001B[0m                             f'not {s.__class__.__name__}')\n",
       "\u001B[1;32m    341\u001B[0m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdetect_encoding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'surrogatepass'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: the JSON object must be str, bytes or bytearray, not method"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2522813972922994>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Loading json schema to create DataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m df3 = spark.createDataFrame(\n\u001B[1;32m      5\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n\n\u001B[0;32m/usr/lib/python3.9/json/__init__.py\u001B[0m in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    337\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    338\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbytes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 339\u001B[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001B[0m\u001B[1;32m    340\u001B[0m                             f'not {s.__class__.__name__}')\n\u001B[1;32m    341\u001B[0m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdetect_encoding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'surrogatepass'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: the JSON object must be str, bytes or bytearray, not method",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: the JSON object must be str, bytes or bytearray, not method",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading json schema to create DataFrame\n",
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(schema.json))\n",
    "df3 = spark.createDataFrame(\n",
    "        spark.sparkContext.parallelize(structureData),schemaFromJson)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f14947-7fdb-439a-b152-f04d194e726e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#8. Creación de estructura de objeto StructType a partir de cadena DDL\n",
    "\n",
    "Al igual que cargar una estructura a partir de una cadena JSON, también podemos crearla a partir de una DLL (utilizando la función estática fromDDL() en la clase SQL StructType StructType.fromDDL). También se puede generar DDL a partir de un esquema utilizando toDDL(). printTreeString() en el objeto struct imprime el esquema de forma similar a como lo hace la función printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216eb7b0-46bf-48b1-9728-97cc34bf633b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2522813972922996>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0;31m# Create StructType from DDL String\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0mddlSchemaStr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"`fullName` STRUCT<`first`: STRING, `last`: STRING,`middle`: STRING>,`age` INT,`gender` STRING\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 5\u001B[0;31m \u001B[0mddlSchema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromDDL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mddlSchemaStr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mddlSchema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintTreeString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: type object 'StructType' has no attribute 'fromDDL'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-2522813972922996>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Create StructType from DDL String\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mddlSchemaStr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"`fullName` STRUCT<`first`: STRING, `last`: STRING,`middle`: STRING>,`age` INT,`gender` STRING\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mddlSchema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromDDL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mddlSchemaStr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mddlSchema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintTreeString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: type object 'StructType' has no attribute 'fromDDL'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: type object 'StructType' has no attribute 'fromDDL'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Create StructType from DDL String\n",
    "ddlSchemaStr = \"`fullName` STRUCT<`first`: STRING, `last`: STRING,`middle`: STRING>,`age` INT,`gender` STRING\"\n",
    "ddlSchema = StructType.fromDDL(ddlSchemaStr)\n",
    "ddlSchema.printTreeString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8ac550-b1a9-4a8b-a5d6-751f43d4e00d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#9. Comprobación de la existencia de una columna en un DataFrame\n",
    "\n",
    "Si queremos realizar algunas comprobaciones sobre los metadatos del DataFrame, por ejemplo, si una columna o campo existe en un DataFrame o el tipo de dato de la columna; podemos hacerlo fácilmente utilizando varias funciones sobre SQL StructType y StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6257bb17-ad07-4b64-8f5d-11ce6a9bc4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "\u001B[0;32m<command-769984081120396>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0;31m# Checking Column exists using contains()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfieldNames\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstname\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mStringType\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'contains'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-769984081120396>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Checking Column exists using contains()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfieldNames\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mStructField\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"firstname\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mStringType\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'contains'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'function' object has no attribute 'contains'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking Column exists using contains()\n",
    "print(df.schema.fieldNames.contains(\"firstname\"))\n",
    "print(df.schema.contains(StructField(\"firstname\",StringType,true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969e8faf-431f-420d-aba3-60dc1e681c44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#10. Ejemplo completo de PySpark StructType & StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b567aad4-f726-46eb-a43b-49905f561ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3100  |\n|{Michael, Rose, }   |40288|M     |4300  |\n|{Robert, , Williams}|42114|M     |1400  |\n|{Maria, Anne, Jones}|39192|F     |5500  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- OtherInfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |OtherInfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3100, Medium}|\n|{Michael, Rose, }   |{40288, M, 4300, High}  |\n|{Robert, , Williams}|{42114, M, 1400, Low}   |\n|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "\n",
    "updatedDF = df2.withColumn(\"OtherInfo\", \n",
    "    struct(col(\"id\").alias(\"identifier\"),\n",
    "    col(\"gender\").alias(\"gender\"),\n",
    "    col(\"salary\").alias(\"salary\"),\n",
    "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "  )).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)\n",
    "\n",
    "\n",
    "\"\"\" Array & Map\"\"\"\n",
    "\n",
    "\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),StringType()), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f16d92cb-c764-43f6-a96f-b67b44d02a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark StructType & StructField",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
