{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6beff26-5a1d-4189-98a0-0c94a9776e42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "(Spark con Python) PySpark DataFrame se puede convertir a Python pandas DataFrame utilizando una función toPandas(), En este artículo, voy a explicar cómo crear Pandas DataFrame de PySpark (Spark) DataFrame con ejemplos.\n",
    "Antes de empezar primero hay que entender las principales diferencias entre Pandas y PySpark, las operaciones en Pyspark se ejecutan más rápido que en Pandas debido a su naturaleza distribuida y ejecución paralela en múltiples núcleos y máquinas.\n",
    "En otras palabras, Pandas ejecuta las operaciones en un único nodo mientras que PySpark lo hace en múltiples máquinas. Si estás trabajando en una aplicación de Aprendizaje Automático donde estás tratando con grandes conjuntos de datos, PySpark procesa operaciones muchas veces más rápido que pandas. Ver pandas DataFrame Tutorial guía para principiantes con ejemplos\n",
    "Después de procesar los datos en PySpark necesitaremos convertirlos de nuevo a Pandas DataFrame para un posterior procesado con la aplicación de Machine Learning o cualquier aplicación Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0fe95e-86a2-4dd2-a60f-e54c2458f2aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41ecef2-b74e-470a-8e4c-d2b60387ce66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkByExamples\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a05640-c014-477d-b85a-2945b8a4b42c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac574a6-863c-4abe-b9fd-ab7ee374c830",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Convertir PySpark Dataframe a Pandas DataFrame\n",
    "PySpark DataFrame proporciona un método toPandas() para convertirlo en Python Pandas DataFrame.\n",
    "\n",
    "toPandas() resulta en la colección de todos los registros en el PySpark DataFrame al programa controlador y debe hacerse sólo en un pequeño subconjunto de los datos. la ejecución en conjuntos de datos más grandes resulta en error de memoria y bloquea la aplicación. Para hacer frente a un conjunto de datos más grande, también puede tratar de aumentar la memoria en el controlador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e2129e-1751-404a-bf0f-b57900bcd645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a33d830-7828-4f29-a221-2e386a58a85d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Convertir Spark Nested Struct DataFrame a Pandas\n",
    "\n",
    "La mayoría de las veces los datos en PySpark DataFrame estarán en un formato estructurado, lo que significa que una columna contiene otras columnas, así que vamos a ver cómo convertirlo a Pandas. Aquí tenemos un ejemplo con una estructura anidada donde tenemos firstname, middlename y lastname como parte de la columna name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df0fdf4-c546-4afa-9caa-7143c5726594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "          StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', StringType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f7decf-914c-4324-922d-b82f549c030b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e68b3dd-37d0-48eb-ba16-6539c601da36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                name    dob gender salary\n0  {'firstname': 'James', 'middlename': '', 'last...  36636      M   3000\n1  {'firstname': 'Michael', 'middlename': 'Rose',...  40288      M   4000\n2  {'firstname': 'Robert', 'middlename': '', 'las...  42114      M   4000\n3  {'firstname': 'Maria', 'middlename': 'Anne', '...  39192      F   4000\n4  {'firstname': 'Jen', 'middlename': 'Mary', 'la...             F     -1\n"
     ]
    }
   ],
   "source": [
    "pandasDF2 = df.toPandas()\n",
    "print(pandasDF2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark - Convert PySpark DataFrame to Pandas",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
